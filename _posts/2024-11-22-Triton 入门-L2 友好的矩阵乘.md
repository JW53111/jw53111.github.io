---
title: Triton 入门-L2 友好的矩阵乘
tags: 学习笔记与作业
---

本文基于 Triton 逐步实现 GEMM 算子。参考了官方 [tutorials](https://github.com/triton-lang/triton/blob/v2.1.0/python/tutorials/03-matrix-multiplication.py)。

## 实验环境

- NVIDIA-A100-PCIE-40GB
- Debian 11
- spack@0.23.0
- cuda@12.6.2
- py-triton@2.1.0
- py-torch@2.4.1+cuda
- py-matplotlib@3.7.5
- py-pandas@1.5.3

## 源代码 `gemm.py`

```python
# spack load py-triton@2.1.0 py-torch@2.4.1+cuda py-matplotlib@3.7.5 py-pandas@1.5.3
# PATH=/usr/sbin:$PATH python3 matmul.py
import triton
import triton.language as tl
import torch


@triton.autotune(
    configs=[
        triton.Config(
            {
                "BLOCK_SIZE_M": 128,
                "BLOCK_SIZE_N": 128,
                "BLOCK_SIZE_K": 32,
                "GROUP_SIZE_M": 0,
            },
            num_stages=4,
            num_warps=4,
        ),
        triton.Config(
            {
                "BLOCK_SIZE_M": 128,
                "BLOCK_SIZE_N": 128,
                "BLOCK_SIZE_K": 32,
                "GROUP_SIZE_M": 8,
            },
            num_stages=4,
            num_warps=4,
        ),
    ],
    key=["M", "N", "K"],  # 这个值的变化会带来 调优配置变化
)
@triton.jit
def kernel_matmul(
    a_ptr,
    b_ptr,
    c_ptr,  # 矩阵指针 a(M, K) * b(K, N) = c(M, N)
    M,
    N,
    K,
    stride_am,
    stride_ak,  # 对于矩阵a来说，stride_am 表示为了访问下一行，需要在a_ptr上相对增加多少
    stride_bk,
    stride_bn,
    stride_cm,
    stride_cn,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    TYPE_C: tl.constexpr,
    TYPE_ACC: tl.constexpr,
):
    pid = tl.program_id(0)
    if GROUP_SIZE_M == 0:  # 不开 L2 友好分块
        pid_m = pid // tl.cdiv(N, BLOCK_SIZE_N)
        pid_n = pid % tl.cdiv(N, BLOCK_SIZE_N)
    else:
        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)

        num_pid_in_group = (
            GROUP_SIZE_M * num_pid_n
        )  # 每个红框里，有多少程序pid; GROUP_SIZE_M是方框在M维度上的尺寸

        group_id = pid // num_pid_in_group  # 本程序所在的group的id， 第几个红框
        first_pid_m = group_id * GROUP_SIZE_M  # 在这个group中，第一个程序的行id
        group_size_m = tl.minimum(
            num_pid_m - first_pid_m, GROUP_SIZE_M
        )  #  预防 num_pid_m 不是 GROUP_SIZE_M 的倍数

        pid_m = first_pid_m + (
            (pid % num_pid_in_group) % group_size_m
        )  # 框中的程序的行 id
        pid_n = (pid % num_pid_in_group) // group_size_m  # 框中的程序的列 id

    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)

    accumulator = tl.zeros([BLOCK_SIZE_M, BLOCK_SIZE_N], dtype=TYPE_ACC)
    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs + k * BLOCK_SIZE_K * stride_ak)
        b = tl.load(b_ptrs + k * BLOCK_SIZE_K * stride_bk)
        accumulator += tl.dot(a, b)
    c = accumulator.to(TYPE_C)

    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn
    tl.store(c_ptrs, c)


def triton_matmul(a: torch.Tensor, b: torch.Tensor):
    assert a.shape[1] == b.shape[0], "Incompatible dimensions"
    assert a.is_contiguous(), "Matrix A must be contiguous"
    assert b.is_contiguous(), "Matrix B must be contiguous"
    assert a.device == b.device
    assert len(a.shape) == 2
    assert len(b.shape) == 2
    M, N, K = a.shape[0], b.shape[1], a.shape[1]
    c = torch.empty([M, N], device=a.device, dtype=torch.result_type(a, b))
    assert c.dtype == torch.float16
    gridDim = lambda META: [
        triton.cdiv(M, META["BLOCK_SIZE_M"]) * triton.cdiv(N, META["BLOCK_SIZE_N"])
    ]
    kernel_matmul[gridDim](
        a,
        b,
        c,
        M,
        N,
        K,
        a.stride(0),
        a.stride(1),
        b.stride(0),
        b.stride(1),
        c.stride(0),
        c.stride(1),
        TYPE_C=tl.float16,
        TYPE_ACC=tl.float32,
    )
    return c


if __name__ == "__main__":
    torch.manual_seed(3407)
    DEVICE = "cuda"  # triton.runtime.driver.active.get_active_torch_device()
    m, n, k = 2**12, 2**11, 2**10
    a = torch.rand([m, k], device=DEVICE, dtype=torch.float16)
    b = torch.rand([k, n], device=DEVICE, dtype=torch.float16)
    torch_c = torch.matmul(a, b)
    triton_c = triton_matmul(a, b)
    print("Max difference: {}".format(torch.max(torch.abs(torch_c - triton_c))))
```

## 程序输出

```plain_text
Max difference: 0.0
```